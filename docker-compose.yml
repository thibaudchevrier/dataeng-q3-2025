# Docker Compose Configuration for Transaction Classification Pipeline
#
# Version Pinning Strategy:
# - All external images use specific version tags to ensure reproducibility
# - Versions are pinned as of January 2026
# - Update versions deliberately after testing to avoid breaking changes
#
# Image Versions:
# - PostgreSQL: 15.10-alpine (LTS, stable - pinned)
# - Flyway: 10.21 (stable migration tool - pinned)
# - Adminer: 4.8.1 (stable database UI - pinned)
# - MinIO: RELEASE.2024-12-18T13-15-44Z (S3-compatible storage - pinned)
# - MinIO Client: latest (CLI tool - MinIO uses non-standard tagging, latest is stable)
# - Apache Kafka: 3.8.1 (stable KRaft mode - pinned to avoid breaking changes in 3.9.x)
# - Kafka UI: v0.7.2 (stable web UI - pinned)
# - Apache Airflow: Built from source with pinned dependencies in Dockerfile
#
# To update versions:
# 1. Check for new stable releases
# 2. Update version tags below
# 3. Test in development: make clean && make all
# 4. Verify all services start and communicate correctly
# 5. Run integration tests: cd pipeline/library && uv run pytest tests
# 6. Commit changes with version update notes

services:
  postgres:
    image: postgres:15.10-alpine
    container_name: transactions-db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:5432"
    # Use tmpfs for ephemeral database (resets on container restart)
    # Comment out for persistent storage, uncomment for clean slate each time
    tmpfs:
      - /var/lib/postgresql/data
    # volumes:
    #   - postgres_data:/var/lib/postgresql/data
    networks:
      - ml-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  flyway:
    image: flyway/flyway:10.21
    container_name: flyway-migrations
    command: -url=jdbc:postgresql://${POSTGRES_HOST}:5432/${POSTGRES_DB} -user=${POSTGRES_USER} -password=${POSTGRES_PASSWORD} -connectRetries=60 migrate
    volumes:
      - ./migrations:/flyway/sql
    networks:
      - ml-network
    depends_on:
      postgres:
        condition: service_healthy

  adminer:
    image: adminer:4.8.1
    container_name: adminer-ui
    ports:
      - "8080:8080"
    networks:
      - ml-network
    depends_on:
      - postgres
    environment:
      ADMINER_DEFAULT_SERVER: postgres

  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    container_name: minio-storage
    ports:
      - "${MINIO_PORT}:9000"  # API
      - "${MINIO_CONSOLE_PORT}:9001"  # Console UI
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - ml-network
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio-init:
    # Note: MinIO uses dated release tags that may not be available in registries
    # Using 'latest' is acceptable for init container as it's not mission-critical
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - ml-network
    volumes:
      - ./data:/data
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://${MINIO_HOST}:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      mc mb myminio/${MINIO_BUCKET} --ignore-existing;
      mc cp /data/transactions_fr.csv myminio/${MINIO_BUCKET}/transactions_fr.csv;
      echo 'Data uploaded to MinIO successfully';
      "

  ml-api:
    build:
      context: ml_api
      dockerfile: Dockerfile
    container_name: ml-api-service
    ports:
      - "8000:8000"
    volumes:
      - ./ml_api/:/app
    command:
      python -m src.main
    networks:
      - ml-network
    depends_on:
      postgres:
        condition: service_healthy

  kafka:
    profiles: ["streaming", "all"]
    image: apache/kafka:3.8.1
    container_name: kafka-broker
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # KRaft mode configuration (no Zookeeper)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      # Two listeners: INTERNAL for Docker, EXTERNAL for localhost
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
    networks:
      - ml-network
    # Simplified healthcheck - just check if port is listening
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s

  kafka-ui:
    profiles: ["streaming", "all"]
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8081:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:19092
    networks:
      - ml-network
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  kafka-init:
    profiles: ["streaming", "all"]
    image: apache/kafka:latest
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - ml-network
    command: >
      bash -c "
      echo 'Kafka is healthy, creating topics...';
      sleep 5;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:19092 --create --if-not-exists --topic transactions --partitions 3 --replication-factor 1;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:19092 --create --if-not-exists --topic predictions --partitions 3 --replication-factor 1;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:19092 --create --if-not-exists --topic failed-transactions --partitions 3 --replication-factor 1;
      echo 'Topics created:';
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:19092 --list;
      "

  # Batch Processor - Build the image so Airflow DockerOperator can use it
  batch-processor:
    profiles: ["batch", "all"]
    build:
      context: .
      dockerfile: pipeline/application/batch/service/Dockerfile
    image: batch-processor:latest
    container_name: batch-processor
    # This service only needs to build the image, not run continuously
    # The DockerOperator will spawn it on demand
    command: "true"
    restart: "no"
    networks:
      - ml-network

  # Airflow - Simple standalone deployment with DockerOperator
  airflow:
    profiles: ["batch", "all"]
    build:
      context: .
      dockerfile: pipeline/application/batch/orchestration/Dockerfile
    container_name: airflow-standalone
    environment:
      # Use SQLite for simplicity (no separate DB needed)
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # Enable standalone mode (webserver + scheduler in one)
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      # Airflow Connections (format: AIRFLOW_CONN_{CONN_ID})
      AIRFLOW_CONN_POSTGRES_TRANSACTIONS: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}
      AIRFLOW_CONN_MINIO_S3: http://${MINIO_ROOT_USER}:${MINIO_ROOT_PASSWORD}@${MINIO_HOST}:9000
      AIRFLOW_CONN_ML_API: http://${ML_API_HOST}:${ML_API_PORT}
    volumes:
      - ./pipeline/application/batch/orchestration/dags:/opt/airflow/dags
      - airflow-data:/opt/airflow
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8082:8080"
    networks:
      - ml-network
    command: >
      bash -c "
      airflow db migrate &&
      mkdir -p /opt/airflow &&
      echo '{\"admin\": \"airflow123\"}' > /opt/airflow/simple_auth_manager_passwords.json &&
      echo '{\"admin\": \"airflow123\"}' > /opt/airflow/simple_auth_manager_passwords.json.generated &&
      airflow standalone
      "
    depends_on:
      batch-processor:
        condition: service_completed_successfully
      flyway:
        condition: service_completed_successfully
      adminer:
        condition: service_started
      minio-init:
        condition: service_completed_successfully
      ml-api:
        condition: service_started
    restart: unless-stopped


  streaming-producer-1:
    profiles: ["streaming", "all"]
    build:
      context: .
      dockerfile: pipeline/application/streaming/producer/Dockerfile
    container_name: streaming-producer-1
    environment:
      ENDPOINT_URL: ${MINIO_ENDPOINT_URL}
      KEY: ${MINIO_ROOT_USER}
      SECRET: ${MINIO_ROOT_PASSWORD}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      PRODUCE_INTERVAL: ${PRODUCER_1_INTERVAL}
      MIN_RECORDS_PER_BATCH: ${PRODUCER_1_MIN_RECORDS}
      MAX_RECORDS_PER_BATCH: ${PRODUCER_1_MAX_RECORDS}
      PRODUCER_RUN_ID: ${PRODUCER_1_RUN_ID}
    networks:
      - ml-network
    depends_on:
      kafka-ui:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped

  streaming-producer-2:
    profiles: ["streaming", "all"]
    build:
      context: .
      dockerfile: pipeline/application/streaming/producer/Dockerfile
    container_name: streaming-producer-2
    environment:
      ENDPOINT_URL: ${MINIO_ENDPOINT_URL}
      KEY: ${MINIO_ROOT_USER}
      SECRET: ${MINIO_ROOT_PASSWORD}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      PRODUCE_INTERVAL: ${PRODUCER_2_INTERVAL}
      MIN_RECORDS_PER_BATCH: ${PRODUCER_2_MIN_RECORDS}
      MAX_RECORDS_PER_BATCH: ${PRODUCER_2_MAX_RECORDS}
      PRODUCER_RUN_ID: ${PRODUCER_2_RUN_ID}
    networks:
      - ml-network
    depends_on:
      kafka-ui:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped

  streaming-consumer:
    profiles: ["streaming", "all"]
    build:
      context: .
      dockerfile: pipeline/application/streaming/consumer/Dockerfile
    container_name: streaming-consumer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_CONSUMER_GROUP: ${KAFKA_CONSUMER_GROUP}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      ML_API_URL: ${ML_API_URL}
      DATABASE_URL: ${DATABASE_URL}
      MESSAGE_BATCH_SIZE: ${STREAMING_MESSAGE_BATCH_SIZE}
      API_BATCH_SIZE: ${STREAMING_API_BATCH_SIZE}
      API_MAX_WORKERS: ${STREAMING_API_MAX_WORKERS}
      DB_ROW_BATCH_SIZE: ${STREAMING_DB_ROW_BATCH_SIZE}
      BUFFER_TIMEOUT: ${STREAMING_BUFFER_TIMEOUT}
    networks:
      - ml-network
    depends_on:
      flyway:
        condition: service_completed_successfully
      adminer:
        condition: service_started
      ml-api:
        condition: service_started
      kafka-ui:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    restart: unless-stopped

volumes:
  #postgres_data:
  minio_data:
  airflow-data:

networks:
  ml-network:
    driver: bridge